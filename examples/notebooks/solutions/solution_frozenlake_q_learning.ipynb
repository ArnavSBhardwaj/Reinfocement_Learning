{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "setup-instructions",
   "metadata": {},
   "source": [
    "# SOLUTION: Q-Learning on FrozenLake\n",
    "\n",
    "THIS NOTEBOOK SHOWS THE SOLUTION TO THE Q-LEARNING ON FROZENLAKE EXERCISE.\n",
    "\n",
    "--- \n",
    "\n",
    "You've already played around with Q-learning in RL Lab, but now let's implement it ourselves!\n",
    "\n",
    "The goal is to get familiar with the gym library and implement the core algorithm, so you can understand what's happening in the backend of RL Lab.\n",
    "\n",
    "## ‚öôÔ∏è Setup Instructions\n",
    "\n",
    "**Before running this notebook:**\n",
    "\n",
    "1. Make sure you've installed dependencies:\n",
    "   ```bash\n",
    "   cd examples\n",
    "   uv sync\n",
    "   ```\n",
    "\n",
    "2. Register the Jupyter kernel (first time only):\n",
    "   ```bash\n",
    "   uv run python -m ipykernel install --user --name=workshop-rl1-examples --display-name \"Python (RL Workshop)\"\n",
    "   ```\n",
    "\n",
    "3. **Select the correct kernel in VS Code:**\n",
    "   - Click \"Select Kernel\" (top right)\n",
    "   - Click \"Jupyter Kernel\"\n",
    "   - Select **\"Python (RL Workshop)\"**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b255ad",
   "metadata": {},
   "source": [
    "\n",
    "### 0. Get familiar with the gym library and its FrozenLake environment\n",
    "\n",
    "Gymnasium (gym) is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. \n",
    "The documentation website is at [gymnasium.farama.org](https://gymnasium.farama.org). More info can be found on their [GitHub page](https://github.com/Farama-Foundation/Gymnasium) and by joining their [Discord server](https://discord.gg/bnJ6kubTg6).\n",
    "\n",
    "In the next cell, we import gym and create the FrozenLake environment. We play a single episode with random actions to see how it works. \n",
    "To see, which arguments are valid for `gym.make('FrozenLake-v1')`, check the [documentation](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**!!! IMPORTANT for MacOS !!!**\n",
    "\n",
    "The render_mode argument in `gym.make` determines how the environment is visualized. \n",
    "`render_mode='human'` opens a pygame window to visualize the environment live while the agent interacts with it.\n",
    "\n",
    "On MacOS, there are known issues with pygame. \n",
    "\n",
    "For example, you may not be able to close the window by clicking the 'X' button. Instead, you may need to force quit the Python process. \n",
    "FOR NOW, JUST KEEP THE WINDOW RUNNING IN THE BACKGROUND AND CONTINUE WITH THE NOTEBOOK.\n",
    "\n",
    "We will use other render modes later, for example 'rgb_array', which returns image frames as numpy arrays, useful for video playback.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create FrozenLake environment (non-slippery for easier learning)\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='human')\n",
    "# ---------------------------------------------------\n",
    "# !!! IMPORTANT for MacOS !!!\n",
    "# The render_mode argument in gym.make determines how the environment is visualized. \n",
    "# render_mode='human' opens a pygame window to visualize the environment live while the agent interacts with it.. On MacOS, there are known issues with pygame. \n",
    "# For example, you may not be able to close the window by clicking the 'X' button. Instead, you may need to force quit the Python process. \n",
    "# FOR NOW, JUST KEEP THE WINDOW RUNNING IN THE BACKGROUND AND CONTINUE WITH THE NOTEBOOK.\n",
    "# We will use other render modes later, for example 'rgb_array', which returns image frames as numpy arrays, useful for video playback.\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Get environment dimensions\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print(f\"FrozenLake Environment:\")\n",
    "print(f\"  States: {num_states} (4x4 grid)\")\n",
    "print(f\"  Actions: {num_actions} (LEFT=0, DOWN=1, RIGHT=2, UP=3)\")\n",
    "print(f\"\\nGoal: Navigate from Start (S) to Goal (G) without falling in Holes (H)\")\n",
    "\n",
    "# Reset environment to start a new episode\n",
    "observation, info = env.reset()\n",
    "# observation: what the agent can \"see\" - here, it's the current state (0-15)\n",
    "# info: extra debugging information (usually not needed for basic learning)\n",
    "print(f\"Starting observation: {observation}\")\n",
    "\n",
    "# Let's run a single episode with random actions to see how it works\n",
    "episode_over = False\n",
    "total_reward = 0\n",
    "while not episode_over:\n",
    "    # Choose an action: \n",
    "    action = env.action_space.sample()  # Random action for now - real agents will be smarter!\n",
    "\n",
    "    # Take the action and see what happens\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    total_reward += reward\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "print(f\"Episode finished! Total reward: {total_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-header",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "From now on, we will use render_mode='rgb_array' to avoid issues for macOS users. In the next cell, we recreate the environment but now also with a new map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using the default 4x4 map, we can create our own custom map as follows:\n",
    "# S = Start, F = Frozen (safe), H = Hole (fall to your doom), G = Goal\n",
    "# To make things more interesting, we use a larger 8x8 map here.\n",
    "custom_map = [\n",
    "    \"SFFHFHFF\",\n",
    "    \"FFFFFFHF\",\n",
    "    \"FFFHFFFF\",\n",
    "    \"FFFFFHFF\",\n",
    "    \"FFFHFFFF\",\n",
    "    \"FHHFFFHF\",\n",
    "    \"FHFFHFHF\",\n",
    "    \"FFFHFFFG\",\n",
    "]\n",
    "\n",
    "# To use the standard 4x4 map, change the desc argument to default_map below\n",
    "default_map = [\n",
    "    \"SFFF\",\n",
    "    \"FHFH\",\n",
    "    \"FFFH\",\n",
    "    \"HFFG\"\n",
    "]\n",
    "\n",
    "# Create FrozenLake environment with 'rgb_array' render mode and custom map\n",
    "env = gym.make('FrozenLake-v1', desc=custom_map, is_slippery=False, render_mode='rgb_array')\n",
    "\n",
    "# Get environment dimensions\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print(f\"FrozenLake Environment:\")\n",
    "print(f\"  States: {num_states} (4x4 grid)\")\n",
    "print(f\"  Actions: {num_actions} (LEFT=0, DOWN=1, RIGHT=2, UP=3)\")\n",
    "print(f\"\\nGoal: Navigate from Start (S) to Goal (G) without falling in Holes (H)\")\n",
    "\n",
    "# Visualize our larger custom environment\n",
    "env.reset()\n",
    "frame = env.render()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(frame)\n",
    "plt.title(\"FrozenLake 4x4\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Environment created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-header",
   "metadata": {},
   "source": [
    "## 2. Q-Learning Implementation\n",
    "\n",
    "Q-Learning learns a Q-table where `Q[state, action]` represents the expected reward for taking an action in a state.\n",
    "\n",
    "**The Q-Learning Update Rule:**\n",
    "\n",
    "```\n",
    "Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max Q(s',a') - Q(s,a)]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `Œ±` (alpha) = learning rate (how much to update)\n",
    "- `Œ≥` (gamma) = discount factor (how much to value future rewards)\n",
    "- `r` = immediate reward\n",
    "- `s'` = next state\n",
    "\n",
    "### Helper Function\n",
    "\n",
    "First, let's define a helper to break ties randomly when multiple actions have the same Q-value, and a function to initialize the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "argmax-helper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_random_tiebreak(q_values):\n",
    "    \"\"\"\n",
    "    Select action with highest Q-value, breaking ties randomly. \n",
    "    This function is needed because np.argmax returns the first occurrence of the maximum value, which can bias action selection.\n",
    "    Critical for exploration when Q-values are tied (e.g., all zeros initially).\n",
    "\n",
    "    Input:\n",
    "        q_values: 1D numpy array of Q-values for the current state\n",
    "    Output:\n",
    "        action: selected action index\n",
    "    \"\"\"\n",
    "    max_value = np.max(q_values)\n",
    "    max_actions = np.where(q_values == max_value)[0]\n",
    "    return np.random.choice(max_actions)\n",
    "\n",
    "def initialize_q_table(num_states, num_actions):\n",
    "    \"\"\"\n",
    "    Initialize Q-table. For now, we initialize all Q-values to zero. If desired, you can experiment with different initializations.\n",
    "\n",
    "    Input:\n",
    "        num_states: number of states in the environment\n",
    "        num_actions: number of actions in the environment\n",
    "    Output:\n",
    "        q_table: 2D numpy array of shape (num_states, num_actions) initialized to zeros\n",
    "    \"\"\"\n",
    "    return np.zeros((num_states, num_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task-header",
   "metadata": {},
   "source": [
    "### YOUR TASK: Implement the Q-Learning Update\n",
    "\n",
    "Fill in the function below to implement the Q-learning update rule. This is the **heart of the algorithm**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q-update-task",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_update(q_table, state, action, reward, next_state, learning_rate, discount_factor):\n",
    "    \"\"\"\n",
    "    Update Q-table using the Q-learning rule.\n",
    "    \n",
    "    TODO: Implement the Q-learning update equation:\n",
    "    Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max(Q(s',a')) - Q(s,a)]\n",
    "    \n",
    "    Hints:\n",
    "    1. Find the maximum Q-value for the next state: best_next_q = argmax_random_tiebreak(q_table[next_state])\n",
    "    2. Calculate TD target: td_target = r + Œ≥¬∑max(Q(s',a'))\n",
    "    3. Calculate TD error: td_error = td_target - Q(s,a)\n",
    "    4. Update Q-value: q_table[state, action] += Œ± * td_error\n",
    "\n",
    "    Input:\n",
    "        q_table: 2D numpy array of Q-values\n",
    "        state: current state (s)\n",
    "        action: action taken (a)\n",
    "        reward: reward received (r)\n",
    "        next_state: next state after taking action a (s')\n",
    "        learning_rate: Œ± (alpha) - step size for updating Q-values\n",
    "        discount_factor: Œ≥ (gamma) - discount factor for future rewards\n",
    "    Output:\n",
    "        None (the Q-table is updated in place)\n",
    "    \"\"\"\n",
    "    # Find the best action for the next state\n",
    "    best_next_action = argmax_random_tiebreak(q_table[next_state])\n",
    "    # Get the Q-value for that best action\n",
    "    best_next_q = q_table[next_state, best_next_action]\n",
    "    # calculate TD target - this is the new information we got\n",
    "    td_target = reward + discount_factor * best_next_q\n",
    "    # calculate TD error - how much we were off compared to our previous estimate\n",
    "    td_error = td_target - q_table[state, action]\n",
    "    # update Q-value - the learning rate governs how much we adjust our estimate (smaller Œ± = slower but more stable learning, higher Œ± = faster but unstable learning) \n",
    "    q_table[state, action] += learning_rate * td_error\n",
    "\n",
    "def epsilon_greedy_action_selection(q_table, state, exploration_rate):\n",
    "    \"\"\"\n",
    "    Select action using epsilon-greedy policy.\n",
    "\n",
    "    TODO: Implement epsilon-greedy action selection.\n",
    "    With probability exploration_rate, select a random action (exploration).\n",
    "    Otherwise, select the best action according to the Q-table (exploitation).\n",
    "\n",
    "    Hints:\n",
    "    1. Generate a random number between 0 and 1: rand = np.random.rand()\n",
    "    2. If rand < exploration_rate, select a random action of the action space (env.action_space.sample()).\n",
    "    3. Otherwise, select the best action using argmax_random_tiebreak on the Q-values for the current state.\n",
    "\n",
    "    Input:\n",
    "        q_table: 2D numpy array of Q-values\n",
    "        state: current state (s)\n",
    "        exploration_rate: Œµ (epsilon) - probability of choosing a random action\n",
    "    Output:\n",
    "        action: selected action index\n",
    "    \"\"\"\n",
    "    # delete next lines before upload\n",
    "    if np.random.rand() < exploration_rate:\n",
    "        return env.action_space.sample()  # Random action\n",
    "    else:\n",
    "        return argmax_random_tiebreak(q_table[state])  # Best action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-header",
   "metadata": {},
   "source": [
    "### Training Loop (Pre-filled)\n",
    "\n",
    "The complete training loop is provided. It uses your `q_learning_update` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, num_episodes=500, learning_rate=0.1, discount_factor=0.95, exploration_rate=0.1):\n",
    "    \"\"\"\n",
    "    Train Q-learning agent on the environment.\n",
    "    \"\"\"\n",
    "    # Initialize Q-table (default to zeros, but if desired, you can experiment with different initializations by changing the function in the cell above) \n",
    "    q_table = initialize_q_table(num_states, num_actions)\n",
    "    episode_rewards = [] # track total rewards per episodes to monitor progress later on\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset() # reset the environment\n",
    "        total_reward = 0 # keep track of total reward per episode. For FrozenLake, rewards are sparse (0 or 1 at the end). For other environments, rewards may be more frequent.\n",
    "        done = False # flag to indicate if episode is over\n",
    "        steps = 0 # count steps taken in the episode\n",
    "        max_steps = 1000  # Prevent infinite loops\n",
    "        \n",
    "        while not done and steps < max_steps:\n",
    "            # Epsilon-greedy action selection\n",
    "            action = epsilon_greedy_action_selection(q_table, state, exploration_rate)\n",
    "            \n",
    "            # Take action and observe outcome\n",
    "            # The env.step function handles the environment dynamics and returns the next state and reward and information about episode termination.\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action) # terminated: episode ended successfully or fell in hole; truncated: episode ended due to time limit\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Update Q-table using your function\n",
    "            q_learning_update(q_table, state, action, reward, next_state, learning_rate, discount_factor)\n",
    "            \n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Progress update every 100 episodes\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward (last 100): {avg_reward:.3f}\")\n",
    "    \n",
    "    return q_table, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-training-header",
   "metadata": {},
   "source": [
    "### Run Training\n",
    "\n",
    "Now let's train the agent! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Q-Learning agent...\\n\")\n",
    "q_table, rewards = train(env, num_episodes=int(5e4), learning_rate=0.1, discount_factor=0.95, exploration_rate=0.1)\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-header",
   "metadata": {},
   "source": [
    "## 3. Results\n",
    "\n",
    "Let's visualize the training progress by plotting the rewards over episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-rewards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rewards, alpha=0.3, label='Episode Reward')\n",
    "\n",
    "# Moving average for smoothing\n",
    "window = 100\n",
    "if len(rewards) >= window:\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(rewards)), moving_avg, linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Q-Learning Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadbb7dd",
   "metadata": {},
   "source": [
    "And now let's play back the learned policy! To avoid the MacOS pygame issues, we will display a video of the agent playing instead of rendering live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7831fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned policy in action\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "def play_episode_with_policy(env, q_table, max_steps=100):\n",
    "    \"\"\"\n",
    "    Play one episode using the learned policy and record frames.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    state, _ = env.reset()\n",
    "    frames.append(env.render())\n",
    "    \n",
    "    done = False\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done and steps < max_steps:\n",
    "        # Select best action (greedy policy, no exploration)\n",
    "        action = argmax_random_tiebreak(q_table[state])\n",
    "        \n",
    "        # Take action\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Record frame\n",
    "        frames.append(env.render())\n",
    "        steps += 1\n",
    "    \n",
    "    return frames, total_reward, done\n",
    "\n",
    "# Play episode and collect frames\n",
    "print(\"Playing episode with learned policy...\")\n",
    "frames, total_reward, success = play_episode_with_policy(env, q_table)\n",
    "\n",
    "print(f\"Episode completed in {len(frames)-1} steps\")\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "print(f\"Success: {'‚úì Reached goal!' if success and total_reward > 0 else '‚úó Failed'}\")\n",
    "\n",
    "# Create animation\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.axis('off')\n",
    "img = ax.imshow(frames[0])\n",
    "\n",
    "def update(frame_idx):\n",
    "    img.set_array(frames[frame_idx])\n",
    "    ax.set_title(f'Step {frame_idx}/{len(frames)-1}')\n",
    "    return [img]\n",
    "\n",
    "anim = FuncAnimation(fig, update, frames=len(frames), interval=300, blit=True, repeat=True)\n",
    "plt.close()  # Prevent static display\n",
    "\n",
    "# Display animation\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show learned policy (best action per state)\n",
    "# Determine grid dimensions from the custom map\n",
    "grid_rows = len(custom_map)\n",
    "grid_cols = len(custom_map[0])\n",
    "\n",
    "policy = np.argmax(q_table, axis=1).reshape(grid_rows, grid_cols)\n",
    "action_symbols = {0: '‚Üê', 1: '‚Üì', 2: '‚Üí', 3: '‚Üë'}\n",
    "\n",
    "print(\"\\nüéØ Learned Policy (best action for each state):\")\n",
    "print(\"\\n\", end=\"\")\n",
    "for i in range(grid_rows):\n",
    "    for j in range(grid_cols):\n",
    "        tile = custom_map[i][j]\n",
    "        \n",
    "        if tile == 'H':\n",
    "            print(\"H  \", end=\"\")\n",
    "        elif tile == 'G':\n",
    "            print(\"G  \", end=\"\")\n",
    "        else:  # `S`(start) and 'F' (frozen)\n",
    "            state = i * grid_cols + j\n",
    "            action = policy[i, j]\n",
    "            print(f\"{action_symbols[action]}  \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've implemented the core of Q-Learning! \n",
    "\n",
    "### Next steps:\n",
    "1. **Challenge**: Modify the code to use `is_slippery=True` - how does performance change?\n",
    "2. Try out other environments in gymnasium, e.g., `Taxi-v3` or `CliffWalking-v0`.\n",
    "3. Explore more advanced algorithms like Deep Q-Networks (DQN) using libraries like Stable Baselines3 on more complex environments. (notebook coming soon!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RL Workshop)",
   "language": "python",
   "name": "workshop-rl1-examples"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
